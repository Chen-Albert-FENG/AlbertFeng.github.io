<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <title>Chen Feng</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/png" href="./imgs/drone.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Chen (Albert) Feng (ÂÜØÂÆ∏)</name></p>
                    I am a Ph.D. candidate in <a href="https://uav.hkust.edu.hk/current-members/">UAV Group</a>, <a href="https://ece.hkust.edu.hk/">ECE</a>
                    at <a href="https://hkust.edu.hk/">HKUST</a>,
                    supervised by <a href="https://uav.hkust.edu.hk/group/"> Prof. Shaojie Shen</a>.
                    Prior to HKUST, I obtained a B.Eng from <a href="http://en.hit.edu.cn/"> Harbin Institute of Technology </a> and majored in Mechatronics Engineering (ME) in 2021.

                    </br></br>
                    I was a research intern from May. 2021 to Sept. 2021 in Video Group at <a href="https://www.megvii.com/megvii_research">Megvii Research</a> of <a href="https://www.megvii.com">Megvii</a> (Beijing, China).
                    In my undergraduate study, I was an team member of computer vision group and mechanics group in <a href="https://hitcrt.com/">
                    Harbin Institute of Technology Competition Robotics Team (HITCRT)</a> (Harbin, China).

                    </br></br>

	            </br>
                </p><p align="center">
                    <a href="mailto:cfengag[-at-]connect.ust.hk">Email</a> /
                    <a href="https://scholar.google.com/citations?user=s71byDsAAAAJ&hl=en">Google Scholar</a> /
                    <a href="https://github.com/Chen-Albert-FENG"> Github </a> /
                    <a href="misc/ChenFENG_CV.pdf"> CV </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 170; height: 225;"></td></tr>
            </tbody>
          </table>
    
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
            <td>
            <heading>Pinned</heading>
            <p align="justify">
            <b>‚úâÔ∏è</b> <em><span style="color: red; font-weight: bold;">I'm expected to graduate in 2025 and am open to both academia and industry positions. If you're interested, please feel free to contact me.</span></em> </p>
            <b>üìå</b> <em><span style="color: black; font-weight: bold;">I'm also seeking self-motivated undergraduate and graduate students for academic collaboration.</span></em>
            </td></tr>
        </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
            <td>
            <heading>News</heading>
            <p align="justify">
            <li><b>2024.08:</b> A paper is conditionally accepted to T-RO.</li> 
            <li><b>2024.06:</b> Three papers are accepted to IROS 2024, all selected as <strong style="color: red;">Oral</strong>.</li> 
            <li><b>2024.06:</b> Invited talk at <a href="https://www.techbeat.net/talk-info?id=880">TechBeat</a> on the field of Aerial Coverage and Reconstruction.</li>
            <li><b>2024.05:</b> A paper is selected as the finalist for ICRA 2024 <strong style="color: red;">Best UAV Paper Award</strong>.</li>
            <li><b>2024.02:</b> A paper is accepted to RA-L.</li> 
            <li><b>2024.01:</b> Three papers are accepted to ICRA 2024.</li> 
            <li><b>2023.12:</b> I have passed Ph.D. Qualifying Examination to become a Ph.D. candidate.</li>  
            <li><b>2023.08:</b> Two papers are accepted to RA-L.</li>
            <li><b>2023.01:</b> A paper is accepted to ICRA 2023.</li>
            <li><b>2022.06:</b> A paper is accepted to CVPR 2022 Workshop.</li>
            </p>
            </td></tr>
        </tbody>
        </table>
    
    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Education</heading>
		   </td></tr>
       </tbody>
    </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    		<tbody><tr>
             <td width="20%" style="vertical-align: middle; text-align: center;"><img src="./imgs/hkust.png" alt="PontTuset" width="95" height="140" style="border-style: none"></td>
                <td width="80%" valign="top">
    	            <p>
                        <b>Ph.D. Candidate</b> | Electronic and Computer Engineering (ECE), HKUST<br>
                        Time: 2021 - Present. Supervisor: <a href="https://uav.hkust.edu.hk/group/">Prof. Shaojie Shen</a>
                    </p>

                    </p><p></p>
                </td>
            </tr>

            <td width="20%" style="vertical-align: middle; text-align: center;"><img src="./imgs/hit.jpeg" alt="PontTuset" width="160" height="140" style="border-style: none"></td>
                <td width="80%" valign="top">
    	            <p>
                        <b>B.Eng</b> | Mechatronics Engineering (ME), HIT<br>
                        Time: 2017 - 2021.
                    </p>

                    </p><p></p>
                </td>
            </tr>

        </tbody>
        </table> 

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
           <p align="justify">  
		   My research focuses on robotics and deep learning, including perception, prediction, and planning. 
           Before HKUST, my research worked on perception and trajectory prediction in autonomous driving. 
           In the present stage, I focus on autonomous and intelligent perception-centric flight including aerial <b>inspection</b>/<b>reconstruction</b>/<b>exploration</b>/<b>coverage</b>, and 3D scene understanding.
		   </p>
		   </td></tr>
       </tbody>
    </table>
    
    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications</heading>
          </td>
	</tr></tbody>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
			<br> (<sup>#</sup> for the corresponding author, * for equal contribution)
        
        <td width="20%"><img src="./imgs/falcon.jpg" alt="PontTuset" width="250" height="120" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="http://www.ieee-ras.org/publications/t-ro">
                <papertitle>FALCON: Fast Autonomous Aerial Exploration using Coverage Path Guidance </papertitle></a>
                <br> Yichen Zhang*, Xinyi Chen*, <strong>Chen Feng</strong>, <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em>IEEE Transactions on Robotics (<strong>T-RO</strong>), 2024. (Conditionally accepted)</em>
                <br>
                <a href="https://arxiv.org/abs/2407.00577"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <!-- <a class="github-button" href="https://github.com/SYSU-STAR/FC-Hetero" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star SYSU-STAR/FC-Hetero on GitHub">Star</a> -->
                <a href="https://www.youtube.com/watch?v=BGH5T2kPbWw"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>
                <!-- <a href="https://sysu-star.github.io/FC-Hetero/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a> -->

                </p><p></p>
                <p align="justify" style="font-size:13px"> This paper introduces <strong>FALCON</strong>, a novel Fast Autonomous expLoration framework using COverage path guidaNce, which aims at setting a new performance benchmark in the field of autonomous aerial exploration. <strong>FALCON</strong> effectively harnesses the full potential of online generated coverage paths in enhancing exploration efficiency. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/FC_Hetero.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://iros2024-abudhabi.org/">
                <papertitle>SOAR: Simultaneous Exploration and Photographing with Heterogeneous UAVs for Fast Autonomous Reconstruction </papertitle></a>
                <br> Mingjie Zhang*, <strong>Chen Feng</strong>*, Zengzhi Li, Guiyong Zheng, Yiming Luo, Zhu Wang, Jinni Zhou, <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>, and <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>.
                <br> <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>),
                2024. </em>
                <br> <em> <strong style="color: red;">Oral</strong>  (<strong style="color: red;">acceptance rate: 10%</strong>)</em>
                <br>
                <a href="https://arxiv.org/abs/2409.02738"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a class="github-button" href="https://github.com/SYSU-STAR/SOAR" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star SYSU-STAR/SOAR on GitHub">Star</a>
                <a href="https://www.bilibili.com/video/BV1ym411B78E/?spm_id_from=333.999.list.card_archive.click&vd_source=0af61c122e5e37c944053b57e313025a"> <img src="https://img.shields.io/badge/BiliBili-Video-blue"> </a>
                <a href="https://github.com/SYSU-STAR/SOAR"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We introduce <strong>SOAR</strong>, a LiDAR-Visual heterogeneous multi-UAV planner that enables simultaneous exploration and photographing for fast autonomous reconstruction of complex scenes.  </p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/omninxt_sys.png" alt="PontTuset" width="250" height="150" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://iros2024-abudhabi.org/">
                <papertitle>OmniNxt: A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception </papertitle></a>
                <br> Peize Liu, <strong>Chen Feng</strong><sup>#</sup>, Yang Xu, Yan Ning, <a href="https://www.xuhao1.me/"> Hao Xu</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>),
                2024. </em>
                <br> <em> <strong style="color: red;">Oral</strong>  (<strong style="color: red;">acceptance rate: 10%</strong>)</em>
                <br>
                <a href="https://arxiv.org/abs/2403.20085"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a class="github-button" href="https://github.com/HKUST-Aerial-Robotics/OmniNxt" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star HKUST-Aerial-Robotics/OmniNxt on GitHub">Star</a>
                <a href="https://www.youtube.com/watch?v=IOuJ7Y6dpeY"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>
                <a href="https://hkust-aerial-robotics.github.io/OmniNxt/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We introduce <strong>OmniNxt</strong>, the <strong>first</strong> fully open-source aerial robotics platform with omnidirectional visual perception. OmniNxt boasts a compact size with exceptional computational resources. Thanks to our meticulous development, we endow OmniNxt with an expansive perception range and efficient resource consumption. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/masstar.jpg" alt="PontTuset" width="250" height="200" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://iros2024-abudhabi.org/">
                <papertitle>MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion </papertitle></a>
                <br> Guiyong Zheng*, Jinqi Jiang*, <strong>Chen Feng</strong>*, <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>, and <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>.
                <br> <em>Tech Report, 2024.</em>
                <br>
                <a href="https://arxiv.org/abs/2403.11681"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a class="github-button" href="https://github.com/SYSU-STAR/MASSTAR" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star SYSU-STAR/MASSTAR on GitHub">Star</a>
                <a href="https://www.youtube.com/watch?v=gqIB08ckjoA"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>
                <a href="https://sysu-star.github.io/MASSTAR/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>MASSTAR</strong>, a Multi-modal lArge-scale Scene dataset with a verSatile Toolchain for surfAce pRediction and completion. We develop a versatile and efficient toolchain for processing the raw 3D data from the environments. </p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/star-serach.png" alt="PontTuset" width="250" height="100" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://www.ieee-ras.org/publications/ra-l">
                <papertitle>Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments</papertitle></a> 
                <br> Yiming Luo, Zixuan Zhuang, Neng Pan, <strong>Chen Feng</strong>, <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>, <a href="http://zju-fast.com/fei-gao/"> Fei Gao</a>, Hui Cheng, and <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>.
                <br> <em>IEEE Robotics and Automation Letters (<strong>RA-L</strong>) with <strong>IROS</strong>, 2024.</em>
                <br>
                <a href="https://arxiv.org/abs/2402.16348"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://ieeexplore.ieee.org/document/10476680"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a class="github-button" href="https://github.com/SYSU-STAR/STAR-Searcher" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star SYSU-STAR/STAR-Searcher on GitHub">Star</a>
                <a href="https://www.bilibili.com/video/BV1Wy4y1c7vj/?spm_id_from=333.999.0.0&vd_source=0af61c122e5e37c944053b57e313025a"> <img src="https://img.shields.io/badge/BiliBili-Video-blue"> </a>

                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>Star-Searcher</strong>, a complete and efficient aerial system for
                    autonomous target search in complex unknown environments. Our aerial system incorporates specialized sensor suites, mapping, and planning modules, all geared towards
                    improving task efficiency and completeness. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/system_overview.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
                <p><a href="https://2024.ieee-icra.org/">
                <papertitle>FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial Coverage of Complex 3D Scenes</papertitle></a>
                <br> <strong>Chen Feng</strong>, Haojia Li, Mingjie Zhang, Xinyi Chen, <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em> IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>),
                2024. </em>
                <br> <em> <strong style="color: red;">Best UAV Paper Award Finalist</strong> </em>
                <br>
                <a href="https://arxiv.org/abs/2309.13882"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://ieeexplore.ieee.org/document/10610621"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a class="github-button" href="https://github.com/HKUST-Aerial-Robotics/FC-Planner" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star HKUST-Aerial-Robotics/FC-Planner on GitHub">Star</a>
                <a href="https://www.youtube.com/watch?v=U-X4OddXI88"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>
                <a href="https://hkust-aerial-robotics.github.io/FC-Planner/"> <img src="https://img.shields.io/badge/Project-Page-green"> </a>
                <a href="https://www.bilibili.com/video/BV1Fr421j7oC/?spm_id_from=333.999.0.0&vd_source=0af61c122e5e37c944053b57e313025a"><img alt="Talk" src="https://img.shields.io/badge/BiliBili-Talk-purple"/></a>


                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose <strong>FC-Planner</strong>, a skeleton-guided planning framework tailored for fast coverage 
                    of large and complex 3D scenes, which results in the generation of high-quality coverage paths and high computational efficiency. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/macformer.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
	            <p><a href="https://www.ieee-ras.org/publications/ra-l">
	            <papertitle>MacFormer: Map-Agent Coupled Transformer for Real-time and Robust Trajectory Prediction</papertitle></a>
                <br> <strong>Chen Feng</strong>, Hangning Zhou, Huadong Lin, Zhigang Zhang, Ziyao Xu, Chi Zhang, <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em> IEEE Robotics and Automation Letters (<strong>RA-L</strong>) with <strong>ICRA</strong>, 2023.</em>
                <br>
                <a href="https://arxiv.org/abs/2308.10280"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://ieeexplore.ieee.org/document/10238733"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a href="https://www.youtube.com/watch?v=XY388iI6sPQ"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>MacFormer</strong>, an one-stage Map-Agent Coupled Transformer for real-time and robust trajectory prediction
                    that explicitly incorporates map constraints into the network achieving state-of-the-art performance with significantly lower inference latency and fewer parameters. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/autotrans.jpg" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
        <td width="80%" valign="top">
	            <p><a href="https://www.ieee-ras.org/publications/ra-l">
	            <papertitle>AutoTrans: A Complete Planning and Control Framework for Autonomous UAV Paylaod Transportation</papertitle></a>
                <br> Haojia Li, Haokun Wang, <strong>Chen Feng</strong>, <a href="http://zju-fast.com/fei-gao/"> Fei Gao</a><sup>#</sup>, <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em> IEEE Robotics and Automation Letters (<strong>RA-L</strong>) with <strong>ICRA</strong>, 2023.</em>
                <br>
                <a href="https://arxiv.org/abs/2310.15050"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b">
                <a href="https://ieeexplore.ieee.org/document/10243043"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a class="github-button" href="https://github.com/HKUST-Aerial-Robotics/AutoTrans" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star HKUST-Aerial-Robotics/AutoTrans on GitHub">Star</a>
                <a href="https://www.youtube.com/watch?v=X9g-ivBqy5g"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>AutoTrans</strong>, a systematic solution for fully autonomous aerial payload transportation
                that includes a real-time planning solution to generate smooth trajectories and an adaptive NMPC with a hierarchical disturbance compensation strategy to overcome unknown external
                perturbations as well as inaccurate model parameters. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/sys_big.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://www.icra2023.org/">
	            <papertitle>PredRecon: A Prediction-boosted Planning Framework for Fast and High-quality Autonomous Aerial Reconstruction</papertitle></a>
                <br> <strong>Chen Feng</strong>, Haojia Li, <a href="http://zju-fast.com/fei-gao/"> Fei Gao</a>, <a href="http://sysu-star.com/"> Boyu Zhou</a><sup>#</sup>, and <a href="https://uav.hkust.edu.hk/group/"> Shaojie Shen</a>.
                <br> <em> IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>), 2023.</em>
                <br>
                <a href="https://ieeexplore.ieee.org/document/10160933"> <img src="https://img.shields.io/badge/IEEE-PDF-b31b1b">
                <a class="github-button" href="https://github.com/HKUST-Aerial-Robotics/PredRecon" data-color-scheme="no-preference: light; light: light; dark: light;" data-show-count="true" aria-label="Star HKUST-Aerial-Robotics/PredRecon on GitHub">Star</a>
                <a href="https://www.youtube.com/watch?v=ek7yY_FZYAc"> <img src="https://img.shields.io/badge/YouTube-Video-blue"> </a>
                <a href="./imgs/ICRA23.pdf"> <img src="https://img.shields.io/badge/Poster-PDF-celery"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>PredRecon</strong>, a prediction-boosted planning framework that can efficiently
reconstruct high-quality 3D models for the target areas in unknown environments with a single flight.</p>
            </td>
        </tr>
        
        <td width="20%"><img src="./imgs/tenet.png" alt="PontTuset" width="250" height="130" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2207.00170">
	            <papertitle>TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction</papertitle></a>
                <br> Yuting Wang*, Hangning Zhou*<sup>,#</sup>, Zhigang Zhang*, <strong>Chen Feng</strong>, Huadong Lin, Chaofei Gao, Yizhi Tang, Zhenting Zhao, Shiyu Zhang, Jie Guo, Xuefeng Wang, Ziyao Xu, and Chi Zhang.
                <br> <em> IEEE/CVF Computer Vision and Pattern Recognition Conference Workshop on Autonomous Driving (<strong>CVPRW</strong>), 2022.</em>
                <br>
                <a href="https://arxiv.org/abs/2207.00170"> <img src="https://img.shields.io/badge/arXiv-PDF-b31b1b"> </a>

                </p><p></p>
			    <p align="justify" style="font-size:13px"> We propose <strong>TENET</strong> to enhance the trajectory temporal encoding via Temporal Flow Header. Besides, an efficient K-means ensemble method is used. Using our Transformer network and ensemble method, we win the <strong> first place of Argoverse 2 Motion Forecasting Challenge </strong> with the
<strong> state-of-the-art </strong> brier-minFDE score of 1.90. </p>
            </td>
        </tr>

    </tbody>
    </table> 

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Honors & Awards</heading>
          <p><b>2024.05</b>, ICRA 2024 Best UAV Paper Award Finalist - IEEE.</p>
          <p><b>2021.09</b>, Postgraduate Studentship - HKUST.</p>
          <p><b>2021.06</b>, Outstanding Graduate - HIT.</p>
          <p><b>2020.08</b>, ABU ROBOCON Robotics Competition First Prize - ABU.</p>
          <p><b>2019.08</b>, National University Students Robotics Competition, RoboMaster First Prize - DJI.</p>
	      <p><b>2018.12</b>, Outstanding Student - HIT.</p>
          <p><b>2018.12</b>, National Scholarship - Ministry of Education, PRC.</p>
		   </td></tr>
       </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td>
           <heading>Talks</heading>
           <p><b>2024.06</b>, <a href="https://www.techbeat.net/talk-info?id=880">Fast and Autonomous Aerial Coverage and Reconstruction of Large-scale Scenes</a> - <a href="https://www.techbeat.net/">TechBeat</a>.</p>
            </td></tr>
        </tbody>
     </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td>
           <heading>Service</heading>
               <p> <strong>Reviewer</strong>: <em>T-ITS, RA-L, ICRA, IROS, CVPR</em></p>
               <p> <strong>Teaching Assistant</strong>: <li>ELEC1100 Introduction to Electro-Robot Design (2023 Fall)</li> 
                <li>ELEC5660 Introduction to Aerial Robots (2024 Spring)</li></p>
            </td></tr>
        </tbody>
     </table>

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
              <p> <strong>Skills</strong>: Python / C++ / Matlab, PyTorch / MegEngine, Linux, ROS, OpenCV, SolidWorks, Adams, ANSYS, Mechanical design</p>
		   </td></tr>
       </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
            <td width="100%" align="middle">
            <p align="center" style="width: 25% ">
            <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=xkVDQxg8nSSD4gBPK_6qDjaYSleekdvNsK5tLhNM_xk"></script>
            </p></td>
        </tr>
        </tbody>
    </table>

    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
		       <p align="right"><font size="2"> Last update: 2024.09.08. <a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
